{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data used to teach\n",
    "train_data = datasets.MNIST(\"./localTrainingData/\", train=True, download=True, transform=transforms.Compose([\n",
    "    # things we want to apply to the data goes in here\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "# data the model hasn't seen, used to test\n",
    "test_data = datasets.MNIST(\"./localTestingData/\", train=False, download=True, transform=transforms.Compose([\n",
    "    # things we want to apply to the data goes in here\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size = how many we want to pass into our model at a time, generally use 8 - 64 - the larger, the quicker we can train, but not too big or accuracy might be low\n",
    "# we have to batch because data is so big we can't fit it all in at once\n",
    "train_set = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_set = torch.utils.data.DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([9, 6, 9, 4, 6, 0, 0, 4, 5, 1])]\n"
     ]
    }
   ],
   "source": [
    "for data in train_set:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "x,y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\mnist-number-identify.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m counter_dict \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m4\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m5\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m6\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m7\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m8\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m9\u001b[39m:\u001b[39m0\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m data \u001b[39min\u001b[39;49;00m train_set:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     Xs, ys \u001b[39m=\u001b[39;49m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m y \u001b[39min\u001b[39;49;00m ys:\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], F_pil\u001b[39m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if model can find shorter path to decreasing loss, it'll take that path\n",
    "# as optimiser is decreasing loss, it doesn't know how low it can get, so it'll try to lower loss as quickly and easily as possible\n",
    "# if majority of data is 3 or 7, it will start to always predict 3, and will be stuck there forever \n",
    "# this is why we need to balance data\n",
    "\n",
    "total = 0\n",
    "counter_dict = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
    "\n",
    "for data in train_set:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i] / total * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers\n",
    "        # fc1 = fully connected layer 1\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        # where 784 = 28 * 28 because our images are 28x28, can't pass the image, we pass the flattened image or just a flat line of pixels\n",
    "        # where 64 = can be whatever we want\n",
    "        # where nn.Linear = just means fully connected\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # because fc1 (first layer) outputs 64, second layer MUST take in 64\n",
    "        # fc2 (second layer) can output whatever we want again\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        # output 10 = because number of classes in our numbers is 10 (0,1,2,...,9)\n",
    "\n",
    "    # simple neural network also called FeedForward network, since data passes from one side to the other, nothing else\n",
    "    # this method defines how data flows through our network\n",
    "    def forward(self, x):\n",
    "        #where x is input\n",
    "\n",
    "        # we just pass x (input) through our layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # where relu = Rectified Linear - this is our activation function\n",
    "        # activation function = whether or not neuron is firing\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        # activation function is being run on the output of each layer. Because layer 4 outputs 10, we want to return probability that a number is what the neural network thinks it is\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = x.view(-1, 28 * 28)\n",
    "# where -1 = specifies this input will be of unknown shape\n",
    "# where 28 = width, height of image being input\n",
    "\n",
    "# this passes info into neural network\n",
    "output = net(X)\n",
    "\n",
    "output\n",
    "# outputs grad_fn = kinda like accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = how wrong model is\n",
    "# we want loss to decrease over time\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "# where net.parameters() = everything that is adjustable in our model\n",
    "# where lr = learning rate = size of increments to take when reducing loss\n",
    "# we can create decaying learning rate system to get to greater accuracy - which we should use in real world scenario\n",
    "\n",
    "EPOCHS = 3 #1 epoch = 1 full pass through our dataset\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in train_set:\n",
    "        #data is batch of featuresets and labels\n",
    "        X, y = data\n",
    "        # where X = image\n",
    "        # where y = label\n",
    "        net.zero_grad()\n",
    "        # start at zero for our gradeints, gradients used in calculating loss\n",
    "        \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        # once we have output, we can calculate how wrong we were (loss)\n",
    "\n",
    "        loss = F.nll_loss(output, y)\n",
    "        # if data is scalar value (like in this case) use nll loss, otherwise if data is a 1-hot-vector, use mean-squared-error\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # adjusts the weights for us\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code:\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_set:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(f\"Accuracy: {round(correct / total, 3) * 100}%\")\n",
    "# produces very high accuracy, in the real world, this is likely not possible so be weary of high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\mnist-number-identify.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(X[index]\u001b[39m.\u001b[39mview(\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/mnist-number-identify.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39margmax(net(X[index]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m))[\u001b[39m0\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "index = 6\n",
    "plt.imshow(X[index].view(28,28))\n",
    "plt.show()\n",
    "print(torch.argmax(net(X[index].view(-1, 28*28))[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
