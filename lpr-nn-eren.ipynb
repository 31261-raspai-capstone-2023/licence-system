{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Anotations and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Download from Roboflow\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"\")\n",
    "# project = rf.workspace(\"augmented-startups\").project(\"vehicle-registration-plates-trudk\")\n",
    "# dataset = project.version(1).download(\"voc\")\n",
    "\n",
    "# Define the source folder containing JPG and XML files\n",
    "source_folder = 'data/roboflow/train'\n",
    "\n",
    "# Define the destination folders\n",
    "images_folder = 'data/roboflow/images'\n",
    "annotations_folder = 'data/roboflow/annotations'\n",
    "\n",
    "# Ensure the destination folders exist; create them if they don't\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "os.makedirs(annotations_folder, exist_ok=True)\n",
    "\n",
    "# Loop through files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    # Check if the file is a JPG image\n",
    "    if filename.lower().endswith('.jpg'):\n",
    "        source_file_path = os.path.join(source_folder, filename)\n",
    "        destination_file_path = os.path.join(images_folder, filename)\n",
    "        # Move the JPG file to the images folder\n",
    "        shutil.move(source_file_path, destination_file_path)\n",
    "    # Check if the file is an XML annotation file\n",
    "    elif filename.lower().endswith('.xml'):\n",
    "        source_file_path = os.path.join(source_folder, filename)\n",
    "        destination_file_path = os.path.join(annotations_folder, filename)\n",
    "        # Move the XML file to the annotations folder\n",
    "        shutil.move(source_file_path, destination_file_path)\n",
    "\n",
    "print(\"Separation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "TESTING_IMAGES_SIZE = 0.1\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_imgs(data):\n",
    "    \"\"\"\n",
    "    Display multiple images with bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A list of lists. Each inner list contains:\n",
    "        [title, image, original_bbox, predicted_bbox]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_imgs = len(data)\n",
    "    fig, axes = plt.subplots(1, num_imgs, figsize=(15, 5 * num_imgs))\n",
    "    \n",
    "    # If there's only one image, axes won't be a list, so we wrap it in a list for consistency\n",
    "    if num_imgs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (title, image, orig_bbox, pred_bbox) in zip(axes, data):\n",
    "        # Handle torch.Tensor\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            if image.is_cuda:\n",
    "                image = image.cpu()  # Move tensor to CPU if it's on CUDA\n",
    "\n",
    "            if image.dim() == 4:  # If a batch of images\n",
    "                image = image[0]  # Take the first image\n",
    "\n",
    "            img = image.detach().permute(1, 2, 0).numpy()  # permute to HWC layout\n",
    "        # Handle PIL.Image\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = np.array(image)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "\n",
    "        # If the image values are between 0 and 1, scale them to [0, 255]\n",
    "        # If the image values are between 0 and 1, scale them to [0, 255]\n",
    "        if isinstance(img, np.ndarray) and img.max() <= 1.0:\n",
    "            img = img * 255\n",
    "            \n",
    "        ax.imshow(img.astype(int), cmap='gray' if len(img.shape) == 2 or img.shape[2] == 1 else None)\n",
    "\n",
    "        # Draw the original bounding box\n",
    "        x, y, x1, y1 = orig_bbox\n",
    "        if isinstance(orig_bbox, torch.Tensor):\n",
    "            x, y, x1, y1 = orig_bbox.cpu().numpy()\n",
    "        rect_orig = patches.Rectangle((x, y), x1 - x, y1 - y, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        ax.add_patch(rect_orig)\n",
    "\n",
    "        # Draw the predicted bounding box\n",
    "        x, y, x1, y1 = pred_bbox\n",
    "        if isinstance(pred_bbox, torch.Tensor):\n",
    "            x, y, x1, y1 = pred_bbox.cpu().numpy()\n",
    "        rect_pred = patches.Rectangle((x, y), x1 - x, y1 - y, linewidth=2, edgecolor='y', facecolor='none')\n",
    "        ax.add_patch(rect_pred)\n",
    "\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class LPR_Training_Dataset_Processed():\n",
    "    IMAGE_PATH = \"data/roboflow/images/\"\n",
    "    ANNOTATION_PATH = \"data/roboflow/annotations/\"\n",
    "\n",
    "    training_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    def create_training_data(self):\n",
    "        self.img_list = os.listdir(self.IMAGE_PATH)\n",
    "        testing_size = len(self.img_list) * TESTING_IMAGES_SIZE\n",
    "\n",
    "        i = 0\n",
    "        for a in tqdm(range(max(2000, len(self.img_list)))):\n",
    "            img_data = self.img_list[a]\n",
    "            img_path = os.path.join(self.IMAGE_PATH, img_data)\n",
    "            annotation_path = os.path.join(self.ANNOTATION_PATH, img_data.replace('.jpg', '.xml')) # get required image annotations\n",
    "            with Image.open(img_path).convert(\"L\") as img:\n",
    "                with open(annotation_path) as source:\n",
    "                    root = ET.parse(source).getroot()\n",
    "                    \n",
    "                    # Iterate through the XML and extract bounding box coordinates\n",
    "                    for obj in root.findall('.//object'):\n",
    "                        bndbox = obj.find('bndbox')\n",
    "                        xmin = int(bndbox.find('xmin').text)\n",
    "                        ymin = int(bndbox.find('ymin').text)\n",
    "                        xmax = int(bndbox.find('xmax').text)\n",
    "                        ymax = int(bndbox.find('ymax').text)\n",
    "                    \n",
    "                    bounding_box_coordinates = (xmin, ymin, xmax, ymax) # resize bounding box to fit resized image\n",
    "                    \n",
    "                    if (i < testing_size):\n",
    "                        #self.testing_data.append([torch.Tensor(np.asarray(img)).view(-1, 416, 416) / 255, torch.Tensor(bounding_box_coordinates)])\n",
    "                        self.testing_data.append([np.asarray(img), bounding_box_coordinates])\n",
    "                    else:\n",
    "                        self.training_data.append([np.asarray(img), bounding_box_coordinates])\n",
    "                        #self.training_data.append([torch.Tensor(np.asarray(img)).view(-1, 416, 416) / 255, torch.Tensor(bounding_box_coordinates)])\n",
    "            i += 1\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        print(f\"Training Images: {len(self.training_data)}\")\n",
    "        print(f\"Testing Images: {len(self.testing_data)}\")\n",
    "\n",
    "training_dataset = LPR_Training_Dataset_Processed()\n",
    "training_dataset.create_training_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LPLocalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LPLocalNet, self).__init__()\n",
    "\n",
    "        # CNNs for grayscale images\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5)\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=192, kernel_size=5)\n",
    "\n",
    "        # Connecting CNN outputs with Fully Connected layers for bounding box\n",
    "        self.box_fc1 = nn.Linear(in_features=12288, out_features=240)\n",
    "        self.box_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.box_out = nn.Linear(in_features=120, out_features=4)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv4(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv5(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.avg_pool2d(t, kernel_size=4, stride=2)\n",
    "\n",
    "        t = torch.flatten(t, start_dim=1)\n",
    "\n",
    "        box_t = self.box_fc1(t)\n",
    "        box_t = F.relu(box_t)\n",
    "\n",
    "        box_t = self.box_fc2(box_t)\n",
    "        box_t = F.relu(box_t)\n",
    "\n",
    "        box_t = self.box_out(box_t)\n",
    "       \n",
    "\n",
    "        return box_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and split between test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "resized_images = [Image.fromarray(i[0]) for i in training_dataset.training_data]\n",
    "numpy_data = np.array([np.array(img.resize((416, 416))) for img in resized_images])\n",
    "X = torch.Tensor(numpy_data)\n",
    "\n",
    "X = X / 255.0\n",
    "\n",
    "numpy_bbox = np.array([\n",
    "    ([416/resized_images[i].width, 416/resized_images[i].height, 416/resized_images[i].width, 416/resized_images[i].height] * np.array(data[1])) for i, data in enumerate(training_dataset.training_data)])\n",
    "y = torch.Tensor(numpy_bbox)\n",
    "\n",
    "VAL_PCT = 0.1 # percent of data we want to use for testing vs training\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "\n",
    "# create test and training splits\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))\n",
    "\n",
    "demo_arr = []\n",
    "for i in range(5):\n",
    "    demo_arr.append([\"Image #{}\".format(i), Image.fromarray(training_dataset.training_data[i][0]), np.array(training_dataset.training_data[i][1]), [0.2, 0.4, 0.2, 0.4] * np.array(training_dataset.training_data[i][1])])\n",
    "show_imgs(demo_arr)\n",
    "demo_arr = []\n",
    "for i in range(5):\n",
    "    orig_img = Image.fromarray(training_dataset.training_data[i][0])\n",
    "    demo_arr.append([\"Image #{}\".format(i), numpy_data[i], numpy_bbox[i], [10, 20, 30, 40]])\n",
    "show_imgs(demo_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from jupyterplot import ProgressPlot\n",
    "\n",
    "def seconds_to_hms(seconds):\n",
    "    h, remainder = divmod(seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    return \"{:02}:{:02}:{:02}\".format(int(h), int(m), int(s))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = LPLocalNet().to(device)\n",
    "print(\"Running\", net.__class__.__name__ ,\"on\", device)\n",
    "\n",
    "BATCH_SIZE = 50 # reduce if memory errors\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.mem_get_info())\n",
    "\n",
    "net.train()\n",
    "pp = ProgressPlot()\n",
    "\n",
    "avg_epoch_duration = None\n",
    "last_loss = 1\n",
    "loss_simple = 1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Optionally shuffle the data here\n",
    "    indices = np.arange(len(train_X))\n",
    "    np.random.shuffle(indices)\n",
    "    train_X = train_X[indices]\n",
    "    train_y = train_y[indices]\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(training_dataset.training_data), BATCH_SIZE), unit=\"batch\"):\n",
    "        batch_X = train_X[i:i + BATCH_SIZE].to(device).view(-1, 1, 416, 416)\n",
    "        batch_y = train_y[i:i + BATCH_SIZE].to(device)\n",
    "        \n",
    "        if (batch_y.shape[0] == 0):\n",
    "            # print(\"Skipping messed up data!\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_stored = (float(loss.cpu().detach().numpy()))\n",
    "        if not np.isnan(loss_stored):\n",
    "            loss_simple = loss_stored\n",
    "    \n",
    "    pp.update(loss_simple)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    if avg_epoch_duration == None:\n",
    "        avg_epoch_duration = epoch_duration\n",
    "    else:\n",
    "        # ikik, its a bad average, but more recent data matters more anyway\n",
    "        avg_epoch_duration += epoch_duration\n",
    "        avg_epoch_duration /= 2\n",
    "\n",
    "    print(\"Epoch #{} - Loss: {} - Loss Diff: {}% - Complete in: {}s\".format(epoch, round(loss_simple, 3), round(-(100 - (loss_simple / last_loss * 100)), 3), seconds_to_hms((EPOCHS - epoch - 1)*avg_epoch_duration)))\n",
    "    last_loss = loss_simple\n",
    "\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "print(\"Total training time: {:.2f} seconds\".format(total_duration))\n",
    "\n",
    "pp.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "ACCEPTABLE_DISTANCE = 15\n",
    "\n",
    "correct_data = []\n",
    "og_correctdata = []\n",
    "\n",
    "net.eval()\n",
    "def close_enough(num1, num2):\n",
    "    a = num1\n",
    "    b = num2\n",
    "    return (abs(a - b) < ACCEPTABLE_DISTANCE)\n",
    "\n",
    "demo_arr = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "       # real_bbox = torch.argmax(test_y[i])\n",
    "        real_bbox = test_y[i]\n",
    "        net_out = net(test_X[i].to(device).view(-1, 1, 416, 416))[0]\n",
    "        #print(net_out)\n",
    "        #predicted_bbox = torch.argmax(net_out)\n",
    "        predicted_bbox = net_out\n",
    "        #print(predicted_bbox[0], real_bbox[0])\n",
    "        # print(\"Comparing\", predicted_bbox, \"with\", real_bbox, \"- Result: \", end=\"\")\n",
    "                    \n",
    "\n",
    "        if close_enough(predicted_bbox[0], real_bbox[0]) and close_enough(predicted_bbox[1], real_bbox[1]) and close_enough(predicted_bbox[2], real_bbox[2]) and close_enough(predicted_bbox[3], real_bbox[3]):\n",
    "            correct+= 1\n",
    "            if len(demo_arr) < 5:\n",
    "                demo_arr.append([\"Image #{}\".format(i), test_X[i].view(-1, 1, 416, 416), real_bbox, predicted_bbox])\n",
    "                print(test_y[i].shape, test_X[i].shape)\n",
    "\n",
    "            correct_data.append([np.asarray(test_y[i].cpu()), predicted_bbox.cpu()])\n",
    "            og_correctdata.append([np.asarray(test_y[i].cpu()), real_bbox.cpu()])\n",
    "            # print(\"Success!\")\n",
    "        # else:\n",
    "            # print(\"Fail.\")\n",
    "        total += 1\n",
    "        #print(real_bbox, net_out)\n",
    "\n",
    "accuracy = round((correct / total) * 100, 3)\n",
    "print(\"Accuracy:\", accuracy, \"%\")\n",
    "show_imgs(demo_arr)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save le model\n",
    "\n",
    "import torch\n",
    "\n",
    "def save_model(model, network_name, batch_size, epochs, learning_rate, accuracy):\n",
    "    \"\"\"\n",
    "    Save the PyTorch model with a filename constructed from training parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The PyTorch model to save\n",
    "    - network_name: Name of the neural network architecture (e.g., \"ResNet50\")\n",
    "    - batch_size: Batch size used during training\n",
    "    - epochs: Number of epochs for training\n",
    "    - learning_rate: Learning rate used during training\n",
    "    - accuracy: Final accuracy of the model\n",
    "\n",
    "    Returns:\n",
    "    - filename: The path to the saved model\n",
    "    \"\"\"\n",
    "    # Construct the filename\n",
    "    filename = f\"{network_name}_B{batch_size}_E{epochs}_LR{learning_rate:.4f}_Acc{accuracy:.2f}.pth\"\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(\"models/checkpoints/\", filename))\n",
    "    \n",
    "    return filename\n",
    "\n",
    "print(\"Saved as:\")\n",
    "save_model(net, net.__class__.__name__, BATCH_SIZE, EPOCHS, LEARNING_RATE, accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
