{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:20<00:00, 20.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training Data Size: 1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class Data_Augmentor():\n",
    "    IMAGE_PATH = \"data/kaggle-dataset-433/train/images\"\n",
    "    IMAGE_OUTPUT_PATH = \"data/kaggle-dataset-433/train/images-processed/\"\n",
    "    ANNOTATION_PATH = \"data/kaggle-dataset-433/train/annotations\"\n",
    "    ANNOTATION_OUTPUT_PATH = \"data/kaggle-dataset-433/train/annotations-processed/\"\n",
    "    def __init__ (self, IMAGE_PATH, ANNOTATION_PATH, OUTPUT_PATH):\n",
    "        self.IMAGE_PATH = IMAGE_PATH\n",
    "        self.IMAGE_OUTPUT_PATH = OUTPUT_PATH\n",
    "        self.ANNOTATION_PATH = ANNOTATION_PATH\n",
    "    \n",
    "    def augment(self):\n",
    "        transform = transforms.ColorJitter(brightness=(0.5,1.5),contrast=(1),saturation=(0.5,1.5),hue=(-0.1,0.1))\n",
    "        transform1 = transforms.GaussianBlur(15)\n",
    "        transform2 = transforms.RandomPerspective(.65)\n",
    "\n",
    "        self.new_images = []\n",
    "        self.new_annotations = []\n",
    "        self.img_list = os.listdir(self.IMAGE_PATH)\n",
    "\n",
    "        # delete all existing processed images\n",
    "       # files = glob.glob(f'{self.IMAGE_OUTPUT_PATH}*')\n",
    "       # print(\"Deleting Existing Files:\")\n",
    "       # for f in tqdm(files):\n",
    "       #     os.remove(f)\n",
    "\n",
    "        print(\"Augmenting Data:\")\n",
    "        for img in tqdm(self.img_list):\n",
    "            img_path = os.path.join(self.IMAGE_PATH, img) #get image file path so we can load it with opencv\n",
    "            annotation_path = os.path.join(self.ANNOTATION_PATH, img.replace('.png', '.xml')) # get required image annotations\n",
    "            tree = ET.parse(annotation_path)\n",
    "            \n",
    "            img = Image.open(img_path)\n",
    "            self.new_images.append(img) # add the existing image without modifications\n",
    "            self.new_annotations.append(tree)\n",
    "\n",
    "            img1 = transform(img)\n",
    "            self.new_images.append(img1)\n",
    "            self.new_annotations.append(tree)\n",
    "\n",
    "            img2 = transform1(img)\n",
    "            self.new_images.append(img2)\n",
    "            self.new_annotations.append(tree)\n",
    "\n",
    "            img3 = transform2(img)\n",
    "            self.new_images.append(img3)\n",
    "            self.new_annotations.append(tree)\n",
    "        \n",
    "        print(f\"New Training Data Size: {len(self.new_images)}\")\n",
    "       # index = 0\n",
    "       # print(\"Saving Files:\")\n",
    "       # for n in tqdm(self.new_images):\n",
    "       #     # other things you need to do snipped\n",
    "       #     n.save(f'{self.IMAGE_OUTPUT_PATH}Cars{index}.png')\n",
    "       #     index += 1\n",
    "        \n",
    "\n",
    "data_aug = Data_Augmentor(IMAGE_PATH=\"data/kaggle-dataset-433/train/images\", ANNOTATION_PATH=\"data/kaggle-dataset-433/train/annotations\", OUTPUT_PATH=\"data/kaggle-dataset-433/train/images-processed/\")      \n",
    "data_aug.augment()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview first few images so we can make sure our data was processed correctly\n",
    "for i in range(0, 8):\n",
    "    img = data_aug.new_images[i]\n",
    "    #x,y,x1,y1 = data_aug.new_annotations\n",
    "    #cv2.rectangle(img, (int(x), int(y)), (int(x1), int(y1)), (255, 255, 255), 2)\n",
    "    cv2.imshow(f\"{i}\", np.asarray(img))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class LPR_Training_Dataset_Processed():\n",
    "    IMAGE_PATH = \"data/kaggle-dataset-433/train/images-processed\"\n",
    "    ANNOTATION_PATH = \"data/kaggle-dataset-433/train/annotations\"\n",
    "    TARGET_IMAGE_SIZE = 224\n",
    "\n",
    "    training_data = []\n",
    "\n",
    "    def create_training_data(self):\n",
    "        self.img_list = os.listdir(self.IMAGE_PATH)\n",
    "        for img in tqdm(self.img_list):\n",
    "            img_path = os.path.join(self.IMAGE_PATH, img) #get image file path so we can load it with opencv\n",
    "            annotation_path = os.path.join(self.ANNOTATION_PATH, img.replace('.png', '.xml')) # get required image annotations\n",
    "\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # read image as grayscale\n",
    "            og_img_height, og_img_width = img.shape # store original shape of image so we can resize boudning box later\n",
    "\n",
    "            img = cv2.resize(img, (self.TARGET_IMAGE_SIZE, self.TARGET_IMAGE_SIZE)) # resize image so they're all the same width and height\n",
    "\n",
    "            # Parse the XML annotation file to extract bounding box coordinates\n",
    "            root = ET.parse(annotation_path).getroot()\n",
    "            \n",
    "            # Iterate through the XML and extract bounding box coordinates\n",
    "            for obj in root.findall('.//object'):\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = int(bndbox.find('xmin').text)\n",
    "                ymin = int(bndbox.find('ymin').text)\n",
    "                xmax = int(bndbox.find('xmax').text)\n",
    "                ymax = int(bndbox.find('ymax').text)\n",
    "            \n",
    "            # calculate new scale ratio\n",
    "            x_scale = self.TARGET_IMAGE_SIZE / og_img_width \n",
    "            y_scale = self.TARGET_IMAGE_SIZE / og_img_height\n",
    "            bounding_box_coordinates = (xmin * x_scale, ymin * y_scale, xmax * x_scale, ymax * y_scale) # resize bounding box to fit resized image\n",
    "            \n",
    "            self.training_data.append([np.array(img), bounding_box_coordinates])\n",
    "            #self.training_data.append(target)\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "\n",
    "training_dataset = LPR_Training_Dataset_Processed()\n",
    "training_dataset.create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview first few images so we can make sure our data was processed correctly\n",
    "for i in range(0, 3):\n",
    "    img = training_dataset.training_data[i][0]\n",
    "    x,y,x1,y1 = training_dataset.training_data[i][1]\n",
    "    cv2.rectangle(img, (int(x), int(y)), (int(x1), int(y1)), (255, 255, 255), 2)\n",
    "    cv2.imshow(f\"{i}\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LPR_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(224, 224).view(-1, 1, 224, 224)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "    \n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None: # used to flatten it since pytorch doesn't have tensorflow's flatten function\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x) # pass through all convulutional layers\n",
    "        x = x.view(-1, self._to_linear) # flatten it\n",
    "        x = F.relu(self.fc1(x)) # pass through fully connected (dense) layer\n",
    "        x = self.fc2(x)\n",
    "        # return F.softmax(x, dim = 1) # renable this when we move to the gpu\n",
    "        return x\n",
    "    \n",
    "net = LPR_Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and split between test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erenc\\AppData\\Local\\Temp\\ipykernel_19508\\929299508.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  X = torch.Tensor([i[0] for i in training_dataset.training_data]).view(-1, 224, 224) # image values\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_dataset.training_data]).view(-1, 224, 224) # image values\n",
    "X = X / 255.0\n",
    "y = torch.Tensor([i[1] for i in training_dataset.training_data]) # bounding box values\n",
    "\n",
    "VAL_PCT = 0.2 # percent of data we want to use for testing vs training\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "\n",
    "# create test and training splits\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:22<00:00,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9973.1279, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100 # reduce if memory errors\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "        batch_X = train_X[i:i + BATCH_SIZE].view(-1, 1, 224, 224)\n",
    "        batch_y = train_y[i:i + BATCH_SIZE]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:02<00:00, 38.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.651 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_bbox = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 224, 224))[0]\n",
    "        predicted_bbox = torch.argmax(net_out)\n",
    "        if predicted_bbox == real_bbox:\n",
    "            correct+= 1\n",
    "        total += 1\n",
    "        #print(real_bbox, net_out)\n",
    "\n",
    "print(\"Accuracy:\", round((correct / total) * 100, 3), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
