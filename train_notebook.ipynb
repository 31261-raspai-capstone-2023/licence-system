{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data: tensor([226., 125., 419., 173.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erenc\\AppData\\Local\\Temp\\ipykernel_13996\\2347565532.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"image\": torch.tensor(image, dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomALPRDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.image_list = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        annotation_path = os.path.join(self.annotation_dir, image_name.replace('.png', '.xml'))\n",
    "\n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
    "\n",
    "        # Parse the XML annotation file to extract bounding box coordinates\n",
    "        # Load the XML annotation file\n",
    "        root = ET.parse(annotation_path).getroot()\n",
    "        \n",
    "        # Iterate through the XML and extract bounding box coordinates\n",
    "        for obj in root.findall('.//object'):\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "        bounding_box_coordinates = (xmin, ymin, xmax, ymax)\n",
    "    \n",
    "        # Preprocess and transform the image (resize, normalize, etc.)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Create a dictionary containing image and target information\n",
    "        target = {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float32),\n",
    "            \"bbox\": torch.tensor(bounding_box_coordinates, dtype=torch.float32),  # Replace with actual bounding box coordinates\n",
    "            \"labels\": torch.tensor([1], dtype=torch.int64),  # Assuming there is only one class (license plate)\n",
    "        }\n",
    "\n",
    "        return target\n",
    "\n",
    "# Define the transform for image preprocessing (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert to PIL Image\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = CustomALPRDataset(image_dir='data/kaggle-dataset-433/train/images', annotation_dir='data/kaggle-dataset-433/train/annotations', transform=transform)\n",
    "\n",
    "print(\"Image Data:\", dataset[0].get(\"bbox\"))\n",
    "\n",
    "# DataLoader can be used for batching and shuffling the dataset during training\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with created dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv2d.__init__() got an unexpected keyword argument 'input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\train_notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Create an instance of the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model \u001b[39m=\u001b[39m MyModel()\n",
      "\u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\train_notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39msuper\u001b[39m(MyModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(input_shape\u001b[39m=\u001b[39;49m(\u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m1\u001b[39;49m), kernel_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, out_channels\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmp2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Conv2d.__init__() got an unexpected keyword argument 'input_shape'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=(28, 28, 1), kernel_size=64, out_channels=(3, 3))\n",
    "        self.mp2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Softmax(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.mp2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Windows not yet supported for torch.compile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\train_notebook.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/erenc/Desktop/_Uni%20Work/Resources/2024_Spring/InternetworkingProject/licence-system/train_notebook.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mcompile()\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2545\u001b[0m, in \u001b[0;36mModule.compile\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2537\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2538\u001b[0m \u001b[39m    Compile this Module's forward using :func:`torch.compile`.\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2543\u001b[0m \u001b[39m    See :func:`torch.compile` for details on the arguments for this function.\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2545\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcompile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\__init__.py:1723\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[0;32m   1720\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1721\u001b[0m     backend \u001b[39m=\u001b[39m _TorchCompileWrapper(backend, mode, options, dynamic)\n\u001b[1;32m-> 1723\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49moptimize(backend\u001b[39m=\u001b[39;49mbackend, nopython\u001b[39m=\u001b[39;49mfullgraph, dynamic\u001b[39m=\u001b[39;49mdynamic, disable\u001b[39m=\u001b[39;49mdisable)(model)\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:583\u001b[0m, in \u001b[0;36moptimize\u001b[1;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    549\u001b[0m     backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minductor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    550\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m     dynamic\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    556\u001b[0m ):\n\u001b[0;32m    557\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39m    The main entrypoint of TorchDynamo.  Do graph capture and call\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m    backend() to optimize extracted graphs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39m            ...\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     check_if_dynamo_supported()\n\u001b[0;32m    584\u001b[0m     \u001b[39m# Note: The hooks object could be global instead of passed around, *however* that would make\u001b[39;00m\n\u001b[0;32m    585\u001b[0m     \u001b[39m# for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     \u001b[39m# There is some prior art around this, w/r/t nesting backend calls are enforced to be the same\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     \u001b[39m# compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[39m# easier to understand UX at the cost of a little more plumbing on our end.\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     hooks \u001b[39m=\u001b[39m Hooks(guard_export_fn\u001b[39m=\u001b[39mguard_export_fn, guard_fail_fn\u001b[39m=\u001b[39mguard_fail_fn)\n",
      "File \u001b[1;32mc:\\Users\\erenc\\Desktop\\_Uni Work\\Resources\\2024_Spring\\InternetworkingProject\\licence-system\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:535\u001b[0m, in \u001b[0;36mcheck_if_dynamo_supported\u001b[1;34m()\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_if_dynamo_supported\u001b[39m():\n\u001b[0;32m    534\u001b[0m     \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 535\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWindows not yet supported for torch.compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    536\u001b[0m     \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m12\u001b[39m):\n\u001b[0;32m    537\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPython 3.12+ not yet supported for torch.compile\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Windows not yet supported for torch.compile"
     ]
    }
   ],
   "source": [
    "model.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
